[
  {
    "objectID": "HW9.html",
    "href": "HW9.html",
    "title": "HW9",
    "section": "",
    "text": "First step we are going to take to create the new models is to just run the relevant code from HW8 in order to simplfy the environment we will be working in. Additionally, some of this code will be taken from the HW8 key to ensure any errors made in my previous code do not cross over.\n\n# Reading in Data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n              local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Changing date column\nbike_data &lt;- bike_data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\n\n# Creating factors\nbike_data &lt;- bike_data |&gt;\n  mutate(season = factor(Seasons),\n    holiday = factor(Holiday), \n    fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`)\n\n# renaming variables\nbike_data &lt;- bike_data |&gt;\nrename('bikes' = `Rented Bike Count`,\n       'hour' = \"Hour\",\n       \"temp\" = `Temperature(°C)`,\n       \"wind\" = `Wind speed (m/s)`,\n       \"humidity\" = `Humidity(%)`,\n       \"vis\" = `Visibility (10m)`,\n       \"dew_point\" = `Dew point temperature(°C)`,\n       \"solar_rads\" = `Solar Radiation (MJ/m2)`,\n       \"rain\" = \"Rainfall(mm)\",\n       \"snow\" = `Snowfall (cm)`)\n\n# removing function day \nbike_data &lt;- bike_data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\n# using group by to find the sum of the bike count, rainfall, and snowfall variables \nbike_data &lt;- bike_data |&gt;\n  group_by(date, season, holiday) |&gt;\n  summarize(bikes = sum(bikes),\n            temp = mean(temp),\n            humidity = mean(humidity),\n            wind = mean(wind),\n            vis = mean(vis),\n            dew_point = mean(dew_point),\n            solar_rads = mean(solar_rads),\n            rain = sum(rain),\n            snow = sum(snow)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'season'. You can override using\nthe `.groups` argument.\n\n# Data check\nbike_data\n\n# A tibble: 353 × 12\n   date       season holiday    bikes    temp humidity  wind   vis dew_point\n   &lt;date&gt;     &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 2017-12-01 Winter No Holiday  9539 -2.45       45.9 1.54  1871.    -13.5 \n 2 2017-12-02 Winter No Holiday  8523  1.32       62.0 1.71  1471.     -5.72\n 3 2017-12-03 Winter No Holiday  7222  4.88       81.5 1.61   456.      1.88\n 4 2017-12-04 Winter No Holiday  8729 -0.304      52.5 3.45  1363.     -9.93\n 5 2017-12-05 Winter No Holiday  8307 -4.46       36.4 1.11  1959.    -17.4 \n 6 2017-12-06 Winter No Holiday  6669  0.0458     70.8 0.696 1187.     -5.19\n 7 2017-12-07 Winter No Holiday  8549  1.09       67.5 1.69   949.     -5.01\n 8 2017-12-08 Winter No Holiday  8032 -3.82       41.8 1.85  1872.    -15.4 \n 9 2017-12-09 Winter No Holiday  7233 -0.846      46   1.08  1861.    -11.2 \n10 2017-12-10 Winter No Holiday  3453  1.19       69.7 2.00  1043.     -4.03\n# ℹ 343 more rows\n# ℹ 3 more variables: solar_rads &lt;dbl&gt;, rain &lt;dbl&gt;, snow &lt;dbl&gt;\n\n# Data split\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = season)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_cv10 &lt;- vfold_cv(bike_train, 10)\n\n# Recipe 1 (only one needed for HW9 as per discussion form)\nMLR_rec &lt;- recipe(bikes ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(weekend_weekday = factor(\n    if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(season, holiday, weekend_weekday) |&gt;\n  step_normalize(all_numeric(), -bikes)\n\n# linear model spec\nMLR_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Model fit using 10 fold CV\nMLR_CV_fit &lt;- workflow() |&gt;\n  add_recipe(MLR_rec) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_cv10)\n\n# Getting metrics\nrbind(MLR_CV_fit |&gt; collect_metrics())\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4284.       10 165.     Preprocessor1_Model1\n2 rsq     standard      0.822    10   0.0151 Preprocessor1_Model1\n\n# Fitting model to training set\nMLR_last_fit &lt;- workflow() |&gt;\n  add_recipe(MLR_rec) |&gt;\n  add_model(MLR_spec) |&gt;\n  last_fit(bike_split)\nMLR_last_fit |&gt;\ncollect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3980.    Preprocessor1_Model1\n2 rsq     standard       0.846 Preprocessor1_Model1\n\n# Final model\nMLR_last_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 14 × 5\n   term                    estimate std.error statistic   p.value\n   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)              17446.       252.    69.3   9.38e-165\n 2 temp                     -2439.      5215.    -0.468 6.40e-  1\n 3 humidity                 -1927.      1904.    -1.01  3.13e-  1\n 4 wind                      -523.       286.    -1.83  6.86e-  2\n 5 vis                        -63.7      361.    -0.177 8.60e-  1\n 6 dew_point                 7143.      6143.     1.16  2.46e-  1\n 7 solar_rads                4088.       473.     8.64  6.74e- 16\n 8 rain                     -1779.       333.    -5.35  2.00e-  7\n 9 snow                      -317.       276.    -1.15  2.50e-  1\n10 season_Spring            -2528.       355.    -7.12  1.14e- 11\n11 season_Summer            -1670.       442.    -3.78  1.98e-  4\n12 season_Winter            -3684.       501.    -7.35  2.88e- 12\n13 holiday_No.Holiday         835.       256.     3.26  1.28e-  3\n14 weekend_weekday_Weekend  -1050.       256.    -4.10  5.56e-  5\n\n\n\n\n\nNow that we have explored the fit of our multiple linear regression models using 10 fold cross validation, we are going to explore what it the fit of a LASSO model will be like. A LASSO model is a Least Angle Subset and Selection Operator, which is similar to the least squares but a penalty is place on the sum of the absolute values of the regression coefficients. Additionally, a (&gt;0) is a tuning parameter and this sets coefficents to 0 as you increase your a (aka shrink).\nTo start we need to create a LASSO recipe then create a spec that includes tune() in linear_reg() as a penalty with a mixture that is equal to 1. The mixture is what actually turns it into a LASSO model (or it would be an elastic net model), while penalty = tune() tells tidymodels we are going to use a resampling method, and glmnet is what allows us to fit a more complicated model\n\n# LASSO recipe (Doing this for clarity s sake even though I could reuse the MLR recipe)\nLASSO_rec &lt;- recipe(bikes ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(weekend_weekday = factor(\n    if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(season, holiday, weekend_weekday) |&gt;\n  step_normalize(all_numeric(), -bikes) # Important note LASSO models should be fit on standardized predictors\n\n# Creating model spec\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nThe next step in the process is to create our workflow which is exactly the same process used to create the MLR workflow.\n\n# Creating LASSO workflow\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(LASSO_rec) |&gt;\n  add_model(LASSO_spec)\nLASSO_wkf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nTo actually fit the model we are going to need to use tune_grid() and grid_regular(), where tune_grid() specifies the values of the tuning parameter, and grid_regular() is a function that chooses a grid based of reasonable values.\n\n# Loading in glmnet\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n# Creating grid \nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = bike_cv10,\n            grid = grid_regular(penalty(), levels = 100),\n            metrics = metric_set(rmse, mae)) # Levels is how many LASSO models you want to create\nLASSO_grid\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [236/27]&gt; Fold01 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [236/27]&gt; Fold02 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [236/27]&gt; Fold03 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [237/26]&gt; Fold04 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [237/26]&gt; Fold05 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [237/26]&gt; Fold06 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [237/26]&gt; Fold07 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [237/26]&gt; Fold08 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [237/26]&gt; Fold09 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [237/26]&gt; Fold10 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nJust as before in the MLR model we need to collect all the metrics across the 100 models we created using collect_metrics(), but to make the values easier to understand we are going to plot it.\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, colour = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nBased off the plot we can see that there is virtually no difference between the RMSE values for our LASSO models, but to get the one with the smallest penalty we will use select_best().\n\nLASSO_lowest &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\nLASSO_lowest\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\nLASSO_best_mae &lt;- LASSO_grid |&gt;\n  select_best(metric = \"mae\")\n\nNow that we have the best one we can use finalize_workflow to tell R to finish the training using the smallest penalty we found in tune(). Then we can fit it to the training model to get our final model fit\n\n# checking to make sure it sets the correct penalty in tune()\nLASSO_wkf |&gt;\n  finalize_workflow(LASSO_lowest)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\n# Creating final model fit\nLASSO_final &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_lowest) |&gt;\n  fit(bike_train)\n\n# Using tidy() to display the model fit\ntidy(LASSO_final)\n\n# A tibble: 14 × 3\n   term                    estimate      penalty\n   &lt;chr&gt;                      &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)               17446. 0.0000000001\n 2 temp                        389. 0.0000000001\n 3 humidity                   -887. 0.0000000001\n 4 wind                       -522. 0.0000000001\n 5 vis                           0  0.0000000001\n 6 dew_point                  3752. 0.0000000001\n 7 solar_rads                 4065. 0.0000000001\n 8 rain                      -1841. 0.0000000001\n 9 snow                       -336. 0.0000000001\n10 season_Spring             -2505. 0.0000000001\n11 season_Summer             -1607. 0.0000000001\n12 season_Winter             -3653. 0.0000000001\n13 holiday_No.Holiday          820. 0.0000000001\n14 weekend_weekday_Weekend   -1060. 0.0000000001\n\n\n\n\n\nTree based methods are a flexible way to split up predictor space into regions. Each of the regions created can have a different prediction made for it. A regression tree is used when the goal is to predict a continuous response, normally using the mean of observations in region as the prediction.\n\n# Making new recipe for clarity\nreg_rec &lt;- LASSO_rec \n\n# Creating decision_tree\nreg_spec &lt;- decision_tree(tree_depth = tune(),\n                           min_n = 20,\n                           cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# Creating workflow\nreg_wkf &lt;- workflow() |&gt;\n  add_recipe(reg_rec) |&gt;\n  add_model(reg_spec)\n\nThe next step is to use CV to select the tuning parameters we will use and to do so we will once again use tune_grid().\n\n# Specifying levels\nreg_grid &lt;- grid_regular(cost_complexity(),\n                         tree_depth(),\n                         levels = c(10, 5))\n\n# Fitting using tune_grid\nreg_fit &lt;- reg_wkf |&gt;\n  tune_grid(resamples = bike_cv10,\n            grid = reg_grid,\n            metrics = metric_set(rmse, mae))\n\nNow that this is setup we should sort this by getting the smallest rmse value, and while doing this will filter just to show rmse. After that we will use select_best() to grab the best tuning parameter values.\n\n# collecting metrics\nreg_fit |&gt;\n  collect_metrics() \n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001          1 mae     standard   4969.    10    243. Preprocess…\n 2    0.0000000001          1 rmse    standard   6432.    10    350. Preprocess…\n 3    0.000000001           1 mae     standard   4969.    10    243. Preprocess…\n 4    0.000000001           1 rmse    standard   6432.    10    350. Preprocess…\n 5    0.00000001            1 mae     standard   4969.    10    243. Preprocess…\n 6    0.00000001            1 rmse    standard   6432.    10    350. Preprocess…\n 7    0.0000001             1 mae     standard   4969.    10    243. Preprocess…\n 8    0.0000001             1 rmse    standard   6432.    10    350. Preprocess…\n 9    0.000001              1 mae     standard   4969.    10    243. Preprocess…\n10    0.000001              1 rmse    standard   6432.    10    350. Preprocess…\n# ℹ 90 more rows\n\n# using select_best()\nreg_best_params &lt;- reg_fit |&gt;\n  select_best(metric = \"rmse\")\nreg_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1           0.001         11 Preprocessor1_Model38\n\nreg_best_mae &lt;- reg_fit |&gt;\n  select_best(metric = \"mae\")\n\nNext step is to finalize the data using finalize_workflow.\n\n# Final fit\nreg_final &lt;- reg_wkf |&gt;\n  finalize_workflow(reg_best_params) |&gt;\n  fit(bike_train)\n\nTo see the way that data is actually fit we can plot the tree\n\n# loading in rpart.plot\nlibrary(rpart.plot)\n\n# Creating plot\nreg_final %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nA bagged tree model is when you use bootstrapping aggregation minus a general method. Bootstrapping is when you resample from the data (non-parametric) or a fitted model (parametric), and have a method or estimation applied to each resample. This can be used to obtain standard errors or construct confidence intervals, but in our case we are going to be looking at standard errors.\n\n# loading in baguette libary\nlibrary(baguette)\n\n# Renaming rec for clarity\nbag_rec &lt;- LASSO_rec\n\n# Setting up model\nbag_spec &lt;- bag_tree(tree_depth = tune(), min_n = 20, cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# Creating workflow\nbag_wkf &lt;- workflow() |&gt;\n  add_recipe(bag_rec) |&gt;\n  add_model(bag_spec)\n\nNow we are going to fit to CV folds as we did to the other models, but of important note this is not really necessary with bagged tree models as we could instead just use out-of-bag observations to determine how well our model is working. We are also going to create a new reg_grid to tune our model.\n\n# Creating fit and grid\nbag_fit &lt;- bag_wkf |&gt;\n  tune_grid(resamples = bike_cv10,\n            grid = grid_regular(cost_complexity(),\n                                tree_depth(),\n                                levels = 5),\n            metrics = metric_set(rmse, mae)) # only doing five levels because it takes a very long time to load\n\n# Collecting metrics across the folds \nbag_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(cost_complexity)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001          1 rmse    standard   6119.    10    371. Preprocess…\n 2    0.0000000001          4 rmse    standard   3591.    10    138. Preprocess…\n 3    0.0000000001          8 rmse    standard   3387.    10    210. Preprocess…\n 4    0.0000000001         11 rmse    standard   3200.    10    182. Preprocess…\n 5    0.0000000001         15 rmse    standard   3222.    10    174. Preprocess…\n 6    0.0000000178          1 rmse    standard   6210.    10    348. Preprocess…\n 7    0.0000000178          4 rmse    standard   3622.    10    151. Preprocess…\n 8    0.0000000178          8 rmse    standard   3289.    10    183. Preprocess…\n 9    0.0000000178         11 rmse    standard   3247.    10    196. Preprocess…\n10    0.0000000178         15 rmse    standard   3389.    10    190. Preprocess…\n# ℹ 15 more rows\n\n\nNow we need to once again use select_best() to grab our best parameter and then fit it on to the training data.\n\n# grabbing best param\nbag_best_params &lt;- bag_fit |&gt;\n  select_best(metric = \"rmse\")\n\nbag_best_mae &lt;- bag_fit |&gt;\n  select_best(metric = \"mae\")\n\n# fitting data\nbag_best &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_params) |&gt;\n  fit(bike_train)\n\nbag_best\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: bag_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                           value  std.error  used\n   &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 temp                    18406679255. 529832464.    11\n 2 dew_point               15128561296. 472668654.    11\n 3 solar_rads              12389817756. 343095894.    11\n 4 season_Winter           11173551984. 464730842.    11\n 5 humidity                 8308758360. 758567969.    11\n 6 rain                     3057553775. 247100185.    11\n 7 wind                     2143504613. 479398257.    11\n 8 snow                     1735161656. 526756766.    10\n 9 season_Summer            1467998524. 560961654.    11\n10 vis                      1433697167. 147766152.    11\n11 season_Spring            1046519002. 103502072.    11\n12 weekend_weekday_Weekend    73248744.  20791849.    10\n13 holiday_No.Holiday         14747782.   9658646.     6\n\n\n\n\n\nThe final model we will be making is a Random forest model, which also uses bagging. It creates multiple trees from the bootstrap samples then it averages the results from them to create a final prediction. The big difference between it and a bagged tree model is that it is does not use all predictors at every step, instead it randomly splits them into a subset for a number of times based off the tuning parameters. If a really strong predictor exists it will likely cause each bootstrap tree to use it for the first split, which makes bagges trees have more correlated predictions.\n\n# Loading in library\nlibrary(ranger)\n\n# renaming recipe for clarity\nrf_rec &lt;- LASSO_rec\n\n# creating model \nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\n# Creating workflow\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_spec)\n\nNow we need to once again set the grid and fit.\n\n# Creating fit and grid\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(resamples = bike_cv10,\n            grid = 10,\n            metrics = metric_set(rmse, mae))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nAfter that we again grab the best params\n\n# grabbing params\nrf_best_params &lt;- rf_fit |&gt;\n  select_best(metric = \"rmse\")\n\nrf_best_mae &lt;- rf_fit |&gt;\n  select_best(metric = \"mae\")\n\n# final fit\nrf_final &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  fit(bike_train)\n\nrf_final\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~11L,      x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5,      1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      263 \nNumber of independent variables:  13 \nMtry:                             11 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       9339504 \nR squared (OOB):                  0.9046045 \n\n\n\n\n\nFirst step is to find which model has the best RMSE and to compare them all in a table to decide upon that.\n\n# final model on LASSO \nLASSO_last_fit &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_lowest) |&gt;\n  last_fit(bike_split)\n\n# collecting metrics\nLASSO_last_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3999.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1\n\n# final model on regression tree\nreg_last_fit &lt;- reg_wkf |&gt;\n  finalize_workflow(reg_best_params) |&gt;\n  last_fit(bike_split)\n\n# collecting metrics\nreg_last_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3096.    Preprocessor1_Model1\n2 rsq     standard       0.905 Preprocessor1_Model1\n\n# final model on bagged tree\nbag_last_fit &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_params) |&gt;\n  last_fit(bike_split)\n\n# collecting metrics\nbag_last_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2963.    Preprocessor1_Model1\n2 rsq     standard       0.915 Preprocessor1_Model1\n\n# final model on random forest model\nrf_last_fit &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse))\n\n# collecting metrics for best rmse\nrbind(LASSO_last_fit |&gt;  collect_metrics(),\n  reg_last_fit |&gt; collect_metrics(),\n  bag_last_fit |&gt; collect_metrics(),\n  rf_last_fit |&gt; collect_metrics(),\n  MLR_last_fit |&gt; collect_metrics())\n\n# A tibble: 9 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3999.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1\n3 rmse    standard    3096.    Preprocessor1_Model1\n4 rsq     standard       0.905 Preprocessor1_Model1\n5 rmse    standard    2963.    Preprocessor1_Model1\n6 rsq     standard       0.915 Preprocessor1_Model1\n7 rmse    standard    2636.    Preprocessor1_Model1\n8 rmse    standard    3980.    Preprocessor1_Model1\n9 rsq     standard       0.846 Preprocessor1_Model1\n\n\nBased off these findings since rf_last_fit has the lowest rmse at 2590 we would choose that as the very best model when selecting based off rmse.\nNow we need to repeat what we just did but instead do it for MAE\n\nLASSO_last_fit2 &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_best_mae) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae))\n\n# final model on regression tree\nreg_last_fit2 &lt;- reg_wkf |&gt;\n  finalize_workflow(reg_best_mae) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae))\n\n# final model on bagged tree\nbag_last_fit2 &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_mae) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae))\n\n# final model on random forest model\nrf_last_fit2 &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_mae) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae))\n\n# collecting metrics for best rmse\nrbind(LASSO_last_fit2 |&gt;  collect_metrics(),\n  reg_last_fit2 |&gt; collect_metrics(),\n  bag_last_fit2 |&gt; collect_metrics(),\n  rf_last_fit2 |&gt; collect_metrics())\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       3063. Preprocessor1_Model1\n2 mae     standard       2362. Preprocessor1_Model1\n3 mae     standard       2431. Preprocessor1_Model1\n4 mae     standard       2121. Preprocessor1_Model1\n\n\nBased off these results we once again find that rf_last_fit has the lowest MAE at 2132 indicating that both the mae and rmse suppourt the notion that this is the best model to fit our data.\n\n\n\nFor the LASSO and MLR models we are going to report the final coefficent tables.\n\n# MLR coefficent table\nMLR_last_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 14 × 5\n   term                    estimate std.error statistic   p.value\n   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)              17446.       252.    69.3   9.38e-165\n 2 temp                     -2439.      5215.    -0.468 6.40e-  1\n 3 humidity                 -1927.      1904.    -1.01  3.13e-  1\n 4 wind                      -523.       286.    -1.83  6.86e-  2\n 5 vis                        -63.7      361.    -0.177 8.60e-  1\n 6 dew_point                 7143.      6143.     1.16  2.46e-  1\n 7 solar_rads                4088.       473.     8.64  6.74e- 16\n 8 rain                     -1779.       333.    -5.35  2.00e-  7\n 9 snow                      -317.       276.    -1.15  2.50e-  1\n10 season_Spring            -2528.       355.    -7.12  1.14e- 11\n11 season_Summer            -1670.       442.    -3.78  1.98e-  4\n12 season_Winter            -3684.       501.    -7.35  2.88e- 12\n13 holiday_No.Holiday         835.       256.     3.26  1.28e-  3\n14 weekend_weekday_Weekend  -1050.       256.    -4.10  5.56e-  5\n\n\nBased off these results we find that dew point temperature had the strongest positive impact on bike counts with an estimate of 7143 and a p- value of 2.46e-1, while the season being winter had the strongest negative impact on bike counts with an estimate of -3684 and a p-value of 2.88e-12.\n\n# LASSO coefficent table\nLASSO_last_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 14 × 3\n   term                    estimate      penalty\n   &lt;chr&gt;                      &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)               17446. 0.0000000001\n 2 temp                        389. 0.0000000001\n 3 humidity                   -887. 0.0000000001\n 4 wind                       -522. 0.0000000001\n 5 vis                           0  0.0000000001\n 6 dew_point                  3752. 0.0000000001\n 7 solar_rads                 4065. 0.0000000001\n 8 rain                      -1841. 0.0000000001\n 9 snow                       -336. 0.0000000001\n10 season_Spring             -2505. 0.0000000001\n11 season_Summer             -1607. 0.0000000001\n12 season_Winter             -3653. 0.0000000001\n13 holiday_No.Holiday          820. 0.0000000001\n14 weekend_weekday_Weekend   -1060. 0.0000000001\n\n\nBased off these results we find that every variable had an extremely low penalty. In this model solar raditaion ending up having the highest positive estimate at 4065, while the season being winter once again had the lowest negative impact at -3653.\nFor the regression tree model we are going to create a plot of the final fit\n\n# regression tree model plot\nreg_last_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint = FALSE, cex = .5)\n\n\n\n\n\n\n\n\nThis plot shows that temp &lt; .034 is an important splitting condition that appears to have a large impact on the quantity of solar radiation when it is not less than that point and when below that it indicates the season is winter. This helps us visualize how each variable is impacting bike counts based off specific values.\nFor the bagged tree and random forest model we are going to make a variable importance plot using the vip package.\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n# rebuilding our model\n\n# creating model \nrf_vip_spec &lt;- rand_forest(mtry = 5, min_n = 5, trees = 100) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")\n\n# Creating workflow\nrf_vip_wkf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_vip_spec)\n\n# creating fit\nrf_vip_fit &lt;- rf_vip_wkf |&gt; \n  last_fit(bike_split)\n\n# creating plot\nrf_vip_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(num_features = 20)\n\n\n\n\n\n\n\n\nAs we can see based off this plot temperature is the most important variable followed by solar radiation.\n\n\n\n\n# Fitting rf_fit to the entire data set\nrf_end_fit &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  fit(bike_data)\nrf_end_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~11L,      x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5,      1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             11 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       7656925 \nR squared (OOB):                  0.9224594"
  },
  {
    "objectID": "HW9.html#fitting-new-models-to-bike-data",
    "href": "HW9.html#fitting-new-models-to-bike-data",
    "title": "HW9",
    "section": "",
    "text": "First step we are going to take to create the new models is to just run the relevant code from HW8 in order to simplfy the environment we will be working in. Additionally, some of this code will be taken from the HW8 key to ensure any errors made in my previous code do not cross over.\n\n# Reading in Data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n              local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Changing date column\nbike_data &lt;- bike_data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\n\n# Creating factors\nbike_data &lt;- bike_data |&gt;\n  mutate(season = factor(Seasons),\n    holiday = factor(Holiday), \n    fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`)\n\n# renaming variables\nbike_data &lt;- bike_data |&gt;\nrename('bikes' = `Rented Bike Count`,\n       'hour' = \"Hour\",\n       \"temp\" = `Temperature(°C)`,\n       \"wind\" = `Wind speed (m/s)`,\n       \"humidity\" = `Humidity(%)`,\n       \"vis\" = `Visibility (10m)`,\n       \"dew_point\" = `Dew point temperature(°C)`,\n       \"solar_rads\" = `Solar Radiation (MJ/m2)`,\n       \"rain\" = \"Rainfall(mm)\",\n       \"snow\" = `Snowfall (cm)`)\n\n# removing function day \nbike_data &lt;- bike_data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\n# using group by to find the sum of the bike count, rainfall, and snowfall variables \nbike_data &lt;- bike_data |&gt;\n  group_by(date, season, holiday) |&gt;\n  summarize(bikes = sum(bikes),\n            temp = mean(temp),\n            humidity = mean(humidity),\n            wind = mean(wind),\n            vis = mean(vis),\n            dew_point = mean(dew_point),\n            solar_rads = mean(solar_rads),\n            rain = sum(rain),\n            snow = sum(snow)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'season'. You can override using\nthe `.groups` argument.\n\n# Data check\nbike_data\n\n# A tibble: 353 × 12\n   date       season holiday    bikes    temp humidity  wind   vis dew_point\n   &lt;date&gt;     &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 2017-12-01 Winter No Holiday  9539 -2.45       45.9 1.54  1871.    -13.5 \n 2 2017-12-02 Winter No Holiday  8523  1.32       62.0 1.71  1471.     -5.72\n 3 2017-12-03 Winter No Holiday  7222  4.88       81.5 1.61   456.      1.88\n 4 2017-12-04 Winter No Holiday  8729 -0.304      52.5 3.45  1363.     -9.93\n 5 2017-12-05 Winter No Holiday  8307 -4.46       36.4 1.11  1959.    -17.4 \n 6 2017-12-06 Winter No Holiday  6669  0.0458     70.8 0.696 1187.     -5.19\n 7 2017-12-07 Winter No Holiday  8549  1.09       67.5 1.69   949.     -5.01\n 8 2017-12-08 Winter No Holiday  8032 -3.82       41.8 1.85  1872.    -15.4 \n 9 2017-12-09 Winter No Holiday  7233 -0.846      46   1.08  1861.    -11.2 \n10 2017-12-10 Winter No Holiday  3453  1.19       69.7 2.00  1043.     -4.03\n# ℹ 343 more rows\n# ℹ 3 more variables: solar_rads &lt;dbl&gt;, rain &lt;dbl&gt;, snow &lt;dbl&gt;\n\n# Data split\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = season)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_cv10 &lt;- vfold_cv(bike_train, 10)\n\n# Recipe 1 (only one needed for HW9 as per discussion form)\nMLR_rec &lt;- recipe(bikes ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(weekend_weekday = factor(\n    if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(season, holiday, weekend_weekday) |&gt;\n  step_normalize(all_numeric(), -bikes)\n\n# linear model spec\nMLR_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Model fit using 10 fold CV\nMLR_CV_fit &lt;- workflow() |&gt;\n  add_recipe(MLR_rec) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_cv10)\n\n# Getting metrics\nrbind(MLR_CV_fit |&gt; collect_metrics())\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4284.       10 165.     Preprocessor1_Model1\n2 rsq     standard      0.822    10   0.0151 Preprocessor1_Model1\n\n# Fitting model to training set\nMLR_last_fit &lt;- workflow() |&gt;\n  add_recipe(MLR_rec) |&gt;\n  add_model(MLR_spec) |&gt;\n  last_fit(bike_split)\nMLR_last_fit |&gt;\ncollect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3980.    Preprocessor1_Model1\n2 rsq     standard       0.846 Preprocessor1_Model1\n\n# Final model\nMLR_last_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 14 × 5\n   term                    estimate std.error statistic   p.value\n   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)              17446.       252.    69.3   9.38e-165\n 2 temp                     -2439.      5215.    -0.468 6.40e-  1\n 3 humidity                 -1927.      1904.    -1.01  3.13e-  1\n 4 wind                      -523.       286.    -1.83  6.86e-  2\n 5 vis                        -63.7      361.    -0.177 8.60e-  1\n 6 dew_point                 7143.      6143.     1.16  2.46e-  1\n 7 solar_rads                4088.       473.     8.64  6.74e- 16\n 8 rain                     -1779.       333.    -5.35  2.00e-  7\n 9 snow                      -317.       276.    -1.15  2.50e-  1\n10 season_Spring            -2528.       355.    -7.12  1.14e- 11\n11 season_Summer            -1670.       442.    -3.78  1.98e-  4\n12 season_Winter            -3684.       501.    -7.35  2.88e- 12\n13 holiday_No.Holiday         835.       256.     3.26  1.28e-  3\n14 weekend_weekday_Weekend  -1050.       256.    -4.10  5.56e-  5\n\n\n\n\n\nNow that we have explored the fit of our multiple linear regression models using 10 fold cross validation, we are going to explore what it the fit of a LASSO model will be like. A LASSO model is a Least Angle Subset and Selection Operator, which is similar to the least squares but a penalty is place on the sum of the absolute values of the regression coefficients. Additionally, a (&gt;0) is a tuning parameter and this sets coefficents to 0 as you increase your a (aka shrink).\nTo start we need to create a LASSO recipe then create a spec that includes tune() in linear_reg() as a penalty with a mixture that is equal to 1. The mixture is what actually turns it into a LASSO model (or it would be an elastic net model), while penalty = tune() tells tidymodels we are going to use a resampling method, and glmnet is what allows us to fit a more complicated model\n\n# LASSO recipe (Doing this for clarity s sake even though I could reuse the MLR recipe)\nLASSO_rec &lt;- recipe(bikes ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(weekend_weekday = factor(\n    if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(season, holiday, weekend_weekday) |&gt;\n  step_normalize(all_numeric(), -bikes) # Important note LASSO models should be fit on standardized predictors\n\n# Creating model spec\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nThe next step in the process is to create our workflow which is exactly the same process used to create the MLR workflow.\n\n# Creating LASSO workflow\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(LASSO_rec) |&gt;\n  add_model(LASSO_spec)\nLASSO_wkf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nTo actually fit the model we are going to need to use tune_grid() and grid_regular(), where tune_grid() specifies the values of the tuning parameter, and grid_regular() is a function that chooses a grid based of reasonable values.\n\n# Loading in glmnet\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n# Creating grid \nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = bike_cv10,\n            grid = grid_regular(penalty(), levels = 100),\n            metrics = metric_set(rmse, mae)) # Levels is how many LASSO models you want to create\nLASSO_grid\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [236/27]&gt; Fold01 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [236/27]&gt; Fold02 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [236/27]&gt; Fold03 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [237/26]&gt; Fold04 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [237/26]&gt; Fold05 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [237/26]&gt; Fold06 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [237/26]&gt; Fold07 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [237/26]&gt; Fold08 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [237/26]&gt; Fold09 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [237/26]&gt; Fold10 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nJust as before in the MLR model we need to collect all the metrics across the 100 models we created using collect_metrics(), but to make the values easier to understand we are going to plot it.\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, colour = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nBased off the plot we can see that there is virtually no difference between the RMSE values for our LASSO models, but to get the one with the smallest penalty we will use select_best().\n\nLASSO_lowest &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\nLASSO_lowest\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\nLASSO_best_mae &lt;- LASSO_grid |&gt;\n  select_best(metric = \"mae\")\n\nNow that we have the best one we can use finalize_workflow to tell R to finish the training using the smallest penalty we found in tune(). Then we can fit it to the training model to get our final model fit\n\n# checking to make sure it sets the correct penalty in tune()\nLASSO_wkf |&gt;\n  finalize_workflow(LASSO_lowest)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\n# Creating final model fit\nLASSO_final &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_lowest) |&gt;\n  fit(bike_train)\n\n# Using tidy() to display the model fit\ntidy(LASSO_final)\n\n# A tibble: 14 × 3\n   term                    estimate      penalty\n   &lt;chr&gt;                      &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)               17446. 0.0000000001\n 2 temp                        389. 0.0000000001\n 3 humidity                   -887. 0.0000000001\n 4 wind                       -522. 0.0000000001\n 5 vis                           0  0.0000000001\n 6 dew_point                  3752. 0.0000000001\n 7 solar_rads                 4065. 0.0000000001\n 8 rain                      -1841. 0.0000000001\n 9 snow                       -336. 0.0000000001\n10 season_Spring             -2505. 0.0000000001\n11 season_Summer             -1607. 0.0000000001\n12 season_Winter             -3653. 0.0000000001\n13 holiday_No.Holiday          820. 0.0000000001\n14 weekend_weekday_Weekend   -1060. 0.0000000001\n\n\n\n\n\nTree based methods are a flexible way to split up predictor space into regions. Each of the regions created can have a different prediction made for it. A regression tree is used when the goal is to predict a continuous response, normally using the mean of observations in region as the prediction.\n\n# Making new recipe for clarity\nreg_rec &lt;- LASSO_rec \n\n# Creating decision_tree\nreg_spec &lt;- decision_tree(tree_depth = tune(),\n                           min_n = 20,\n                           cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# Creating workflow\nreg_wkf &lt;- workflow() |&gt;\n  add_recipe(reg_rec) |&gt;\n  add_model(reg_spec)\n\nThe next step is to use CV to select the tuning parameters we will use and to do so we will once again use tune_grid().\n\n# Specifying levels\nreg_grid &lt;- grid_regular(cost_complexity(),\n                         tree_depth(),\n                         levels = c(10, 5))\n\n# Fitting using tune_grid\nreg_fit &lt;- reg_wkf |&gt;\n  tune_grid(resamples = bike_cv10,\n            grid = reg_grid,\n            metrics = metric_set(rmse, mae))\n\nNow that this is setup we should sort this by getting the smallest rmse value, and while doing this will filter just to show rmse. After that we will use select_best() to grab the best tuning parameter values.\n\n# collecting metrics\nreg_fit |&gt;\n  collect_metrics() \n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001          1 mae     standard   4969.    10    243. Preprocess…\n 2    0.0000000001          1 rmse    standard   6432.    10    350. Preprocess…\n 3    0.000000001           1 mae     standard   4969.    10    243. Preprocess…\n 4    0.000000001           1 rmse    standard   6432.    10    350. Preprocess…\n 5    0.00000001            1 mae     standard   4969.    10    243. Preprocess…\n 6    0.00000001            1 rmse    standard   6432.    10    350. Preprocess…\n 7    0.0000001             1 mae     standard   4969.    10    243. Preprocess…\n 8    0.0000001             1 rmse    standard   6432.    10    350. Preprocess…\n 9    0.000001              1 mae     standard   4969.    10    243. Preprocess…\n10    0.000001              1 rmse    standard   6432.    10    350. Preprocess…\n# ℹ 90 more rows\n\n# using select_best()\nreg_best_params &lt;- reg_fit |&gt;\n  select_best(metric = \"rmse\")\nreg_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1           0.001         11 Preprocessor1_Model38\n\nreg_best_mae &lt;- reg_fit |&gt;\n  select_best(metric = \"mae\")\n\nNext step is to finalize the data using finalize_workflow.\n\n# Final fit\nreg_final &lt;- reg_wkf |&gt;\n  finalize_workflow(reg_best_params) |&gt;\n  fit(bike_train)\n\nTo see the way that data is actually fit we can plot the tree\n\n# loading in rpart.plot\nlibrary(rpart.plot)\n\n# Creating plot\nreg_final %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nA bagged tree model is when you use bootstrapping aggregation minus a general method. Bootstrapping is when you resample from the data (non-parametric) or a fitted model (parametric), and have a method or estimation applied to each resample. This can be used to obtain standard errors or construct confidence intervals, but in our case we are going to be looking at standard errors.\n\n# loading in baguette libary\nlibrary(baguette)\n\n# Renaming rec for clarity\nbag_rec &lt;- LASSO_rec\n\n# Setting up model\nbag_spec &lt;- bag_tree(tree_depth = tune(), min_n = 20, cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# Creating workflow\nbag_wkf &lt;- workflow() |&gt;\n  add_recipe(bag_rec) |&gt;\n  add_model(bag_spec)\n\nNow we are going to fit to CV folds as we did to the other models, but of important note this is not really necessary with bagged tree models as we could instead just use out-of-bag observations to determine how well our model is working. We are also going to create a new reg_grid to tune our model.\n\n# Creating fit and grid\nbag_fit &lt;- bag_wkf |&gt;\n  tune_grid(resamples = bike_cv10,\n            grid = grid_regular(cost_complexity(),\n                                tree_depth(),\n                                levels = 5),\n            metrics = metric_set(rmse, mae)) # only doing five levels because it takes a very long time to load\n\n# Collecting metrics across the folds \nbag_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(cost_complexity)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001          1 rmse    standard   6119.    10    371. Preprocess…\n 2    0.0000000001          4 rmse    standard   3591.    10    138. Preprocess…\n 3    0.0000000001          8 rmse    standard   3387.    10    210. Preprocess…\n 4    0.0000000001         11 rmse    standard   3200.    10    182. Preprocess…\n 5    0.0000000001         15 rmse    standard   3222.    10    174. Preprocess…\n 6    0.0000000178          1 rmse    standard   6210.    10    348. Preprocess…\n 7    0.0000000178          4 rmse    standard   3622.    10    151. Preprocess…\n 8    0.0000000178          8 rmse    standard   3289.    10    183. Preprocess…\n 9    0.0000000178         11 rmse    standard   3247.    10    196. Preprocess…\n10    0.0000000178         15 rmse    standard   3389.    10    190. Preprocess…\n# ℹ 15 more rows\n\n\nNow we need to once again use select_best() to grab our best parameter and then fit it on to the training data.\n\n# grabbing best param\nbag_best_params &lt;- bag_fit |&gt;\n  select_best(metric = \"rmse\")\n\nbag_best_mae &lt;- bag_fit |&gt;\n  select_best(metric = \"mae\")\n\n# fitting data\nbag_best &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_params) |&gt;\n  fit(bike_train)\n\nbag_best\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: bag_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                           value  std.error  used\n   &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 temp                    18406679255. 529832464.    11\n 2 dew_point               15128561296. 472668654.    11\n 3 solar_rads              12389817756. 343095894.    11\n 4 season_Winter           11173551984. 464730842.    11\n 5 humidity                 8308758360. 758567969.    11\n 6 rain                     3057553775. 247100185.    11\n 7 wind                     2143504613. 479398257.    11\n 8 snow                     1735161656. 526756766.    10\n 9 season_Summer            1467998524. 560961654.    11\n10 vis                      1433697167. 147766152.    11\n11 season_Spring            1046519002. 103502072.    11\n12 weekend_weekday_Weekend    73248744.  20791849.    10\n13 holiday_No.Holiday         14747782.   9658646.     6\n\n\n\n\n\nThe final model we will be making is a Random forest model, which also uses bagging. It creates multiple trees from the bootstrap samples then it averages the results from them to create a final prediction. The big difference between it and a bagged tree model is that it is does not use all predictors at every step, instead it randomly splits them into a subset for a number of times based off the tuning parameters. If a really strong predictor exists it will likely cause each bootstrap tree to use it for the first split, which makes bagges trees have more correlated predictions.\n\n# Loading in library\nlibrary(ranger)\n\n# renaming recipe for clarity\nrf_rec &lt;- LASSO_rec\n\n# creating model \nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\n# Creating workflow\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_spec)\n\nNow we need to once again set the grid and fit.\n\n# Creating fit and grid\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(resamples = bike_cv10,\n            grid = 10,\n            metrics = metric_set(rmse, mae))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nAfter that we again grab the best params\n\n# grabbing params\nrf_best_params &lt;- rf_fit |&gt;\n  select_best(metric = \"rmse\")\n\nrf_best_mae &lt;- rf_fit |&gt;\n  select_best(metric = \"mae\")\n\n# final fit\nrf_final &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  fit(bike_train)\n\nrf_final\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~11L,      x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5,      1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      263 \nNumber of independent variables:  13 \nMtry:                             11 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       9339504 \nR squared (OOB):                  0.9046045 \n\n\n\n\n\nFirst step is to find which model has the best RMSE and to compare them all in a table to decide upon that.\n\n# final model on LASSO \nLASSO_last_fit &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_lowest) |&gt;\n  last_fit(bike_split)\n\n# collecting metrics\nLASSO_last_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3999.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1\n\n# final model on regression tree\nreg_last_fit &lt;- reg_wkf |&gt;\n  finalize_workflow(reg_best_params) |&gt;\n  last_fit(bike_split)\n\n# collecting metrics\nreg_last_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3096.    Preprocessor1_Model1\n2 rsq     standard       0.905 Preprocessor1_Model1\n\n# final model on bagged tree\nbag_last_fit &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_params) |&gt;\n  last_fit(bike_split)\n\n# collecting metrics\nbag_last_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2963.    Preprocessor1_Model1\n2 rsq     standard       0.915 Preprocessor1_Model1\n\n# final model on random forest model\nrf_last_fit &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse))\n\n# collecting metrics for best rmse\nrbind(LASSO_last_fit |&gt;  collect_metrics(),\n  reg_last_fit |&gt; collect_metrics(),\n  bag_last_fit |&gt; collect_metrics(),\n  rf_last_fit |&gt; collect_metrics(),\n  MLR_last_fit |&gt; collect_metrics())\n\n# A tibble: 9 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3999.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1\n3 rmse    standard    3096.    Preprocessor1_Model1\n4 rsq     standard       0.905 Preprocessor1_Model1\n5 rmse    standard    2963.    Preprocessor1_Model1\n6 rsq     standard       0.915 Preprocessor1_Model1\n7 rmse    standard    2636.    Preprocessor1_Model1\n8 rmse    standard    3980.    Preprocessor1_Model1\n9 rsq     standard       0.846 Preprocessor1_Model1\n\n\nBased off these findings since rf_last_fit has the lowest rmse at 2590 we would choose that as the very best model when selecting based off rmse.\nNow we need to repeat what we just did but instead do it for MAE\n\nLASSO_last_fit2 &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_best_mae) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae))\n\n# final model on regression tree\nreg_last_fit2 &lt;- reg_wkf |&gt;\n  finalize_workflow(reg_best_mae) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae))\n\n# final model on bagged tree\nbag_last_fit2 &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_mae) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae))\n\n# final model on random forest model\nrf_last_fit2 &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_mae) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae))\n\n# collecting metrics for best rmse\nrbind(LASSO_last_fit2 |&gt;  collect_metrics(),\n  reg_last_fit2 |&gt; collect_metrics(),\n  bag_last_fit2 |&gt; collect_metrics(),\n  rf_last_fit2 |&gt; collect_metrics())\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       3063. Preprocessor1_Model1\n2 mae     standard       2362. Preprocessor1_Model1\n3 mae     standard       2431. Preprocessor1_Model1\n4 mae     standard       2121. Preprocessor1_Model1\n\n\nBased off these results we once again find that rf_last_fit has the lowest MAE at 2132 indicating that both the mae and rmse suppourt the notion that this is the best model to fit our data.\n\n\n\nFor the LASSO and MLR models we are going to report the final coefficent tables.\n\n# MLR coefficent table\nMLR_last_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 14 × 5\n   term                    estimate std.error statistic   p.value\n   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)              17446.       252.    69.3   9.38e-165\n 2 temp                     -2439.      5215.    -0.468 6.40e-  1\n 3 humidity                 -1927.      1904.    -1.01  3.13e-  1\n 4 wind                      -523.       286.    -1.83  6.86e-  2\n 5 vis                        -63.7      361.    -0.177 8.60e-  1\n 6 dew_point                 7143.      6143.     1.16  2.46e-  1\n 7 solar_rads                4088.       473.     8.64  6.74e- 16\n 8 rain                     -1779.       333.    -5.35  2.00e-  7\n 9 snow                      -317.       276.    -1.15  2.50e-  1\n10 season_Spring            -2528.       355.    -7.12  1.14e- 11\n11 season_Summer            -1670.       442.    -3.78  1.98e-  4\n12 season_Winter            -3684.       501.    -7.35  2.88e- 12\n13 holiday_No.Holiday         835.       256.     3.26  1.28e-  3\n14 weekend_weekday_Weekend  -1050.       256.    -4.10  5.56e-  5\n\n\nBased off these results we find that dew point temperature had the strongest positive impact on bike counts with an estimate of 7143 and a p- value of 2.46e-1, while the season being winter had the strongest negative impact on bike counts with an estimate of -3684 and a p-value of 2.88e-12.\n\n# LASSO coefficent table\nLASSO_last_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 14 × 3\n   term                    estimate      penalty\n   &lt;chr&gt;                      &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)               17446. 0.0000000001\n 2 temp                        389. 0.0000000001\n 3 humidity                   -887. 0.0000000001\n 4 wind                       -522. 0.0000000001\n 5 vis                           0  0.0000000001\n 6 dew_point                  3752. 0.0000000001\n 7 solar_rads                 4065. 0.0000000001\n 8 rain                      -1841. 0.0000000001\n 9 snow                       -336. 0.0000000001\n10 season_Spring             -2505. 0.0000000001\n11 season_Summer             -1607. 0.0000000001\n12 season_Winter             -3653. 0.0000000001\n13 holiday_No.Holiday          820. 0.0000000001\n14 weekend_weekday_Weekend   -1060. 0.0000000001\n\n\nBased off these results we find that every variable had an extremely low penalty. In this model solar raditaion ending up having the highest positive estimate at 4065, while the season being winter once again had the lowest negative impact at -3653.\nFor the regression tree model we are going to create a plot of the final fit\n\n# regression tree model plot\nreg_last_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint = FALSE, cex = .5)\n\n\n\n\n\n\n\n\nThis plot shows that temp &lt; .034 is an important splitting condition that appears to have a large impact on the quantity of solar radiation when it is not less than that point and when below that it indicates the season is winter. This helps us visualize how each variable is impacting bike counts based off specific values.\nFor the bagged tree and random forest model we are going to make a variable importance plot using the vip package.\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n# rebuilding our model\n\n# creating model \nrf_vip_spec &lt;- rand_forest(mtry = 5, min_n = 5, trees = 100) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")\n\n# Creating workflow\nrf_vip_wkf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_vip_spec)\n\n# creating fit\nrf_vip_fit &lt;- rf_vip_wkf |&gt; \n  last_fit(bike_split)\n\n# creating plot\nrf_vip_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(num_features = 20)\n\n\n\n\n\n\n\n\nAs we can see based off this plot temperature is the most important variable followed by solar radiation.\n\n\n\n\n# Fitting rf_fit to the entire data set\nrf_end_fit &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  fit(bike_data)\nrf_end_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~11L,      x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5,      1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             11 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       7656925 \nR squared (OOB):                  0.9224594"
  }
]